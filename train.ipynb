{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be8db9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /home/ramon/tts\n",
      "DATASET_DIR: /home/ramon/tts/dataset\n",
      "WAV_DIR: /home/ramon/tts/dataset/wavs\n",
      "TEXTY_PATH: /home/ramon/tts/third_party/TextyMcSpeechy\n"
     ]
    }
   ],
   "source": [
    "# TODO: Look at https://github.com/simoniz0r/piper-voice-models/tree/main?tab=readme-ov-file\n",
    "# How he uses training sets from F5-TTS\n",
    "# An got good results\n",
    "# https://community.home-assistant.io/t/collections-of-pretrainer-piper-voices/915666\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import shutil\n",
    "import whisper\n",
    "\n",
    "YOUTUBE_IDS = [\"2hOp408Ib5w\", \"yJzjyYL8l5Y\"]\n",
    "\n",
    "if len(YOUTUBE_IDS) < 2:\n",
    "    print(\"Minimum of 2 youtube videos\")\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "VOICE_NAME = \"custom_voice\"\n",
    "BASE_DIR = os.getcwd()\n",
    "TARGET_SAMPLE = \"wav_22050\"  # wav_16000, wav_22050, wav_44100\n",
    "DATASET_DIR = f\"{BASE_DIR}/dataset\"\n",
    "WAV_DIR = f\"{DATASET_DIR}/wavs\"\n",
    "TEXTY_PATH = f\"{BASE_DIR}/third_party/TextyMcSpeechy\"\n",
    "\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "if os.path.exists(WAV_DIR):\n",
    "    shutil.rmtree(WAV_DIR)\n",
    "os.makedirs(WAV_DIR, exist_ok=True)\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"DATASET_DIR:\", DATASET_DIR)\n",
    "print(\"WAV_DIR:\", WAV_DIR)\n",
    "print(\"TEXTY_PATH:\", TEXTY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "647e71f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtube.com/watch?v=2hOp408Ib5w\n",
      "[youtube] 2hOp408Ib5w: Downloading webpage\n",
      "[youtube] 2hOp408Ib5w: Downloading tv client config\n",
      "[youtube] 2hOp408Ib5w: Downloading tv player API JSON\n",
      "[youtube] 2hOp408Ib5w: Downloading ios player API JSON\n",
      "[youtube] 2hOp408Ib5w: Downloading player 65578ad1-main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/65578ad1/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] 2hOp408Ib5w: nsig extraction failed: Some formats may be missing\n",
      "         n = yJH3tIXlwreq5w1 ; player = https://www.youtube.com/s/player/65578ad1/player_ias.vflset/en_US/base.js\n",
      "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
      "WARNING: [youtube] 2hOp408Ib5w: nsig extraction failed: Some formats may be missing\n",
      "         n = mehidRgMSENs4uG ; player = https://www.youtube.com/s/player/65578ad1/player_ias.vflset/en_US/base.js\n",
      "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
      "WARNING: [youtube] 2hOp408Ib5w: nsig extraction failed: Some formats may be missing\n",
      "         n = WZgpdSyycGXo7lQ ; player = https://www.youtube.com/s/player/65578ad1/player_ias.vflset/en_US/base.js\n",
      "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
      "WARNING: [youtube] 2hOp408Ib5w: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] 2hOp408Ib5w: Downloading m3u8 information\n",
      "[info] 2hOp408Ib5w: Downloading 1 format(s): 234\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 26\n",
      "[download] Destination: /home/ramon/tts/dataset/wavs/0.mp4\n",
      "[download] 100% of    2.02MiB in 00:00:00 at 5.09MiB/s                   \n",
      "[ExtractAudio] Destination: /home/ramon/tts/dataset/wavs/0.wav\n",
      "Deleting original file /home/ramon/tts/dataset/wavs/0.mp4 (pass -k to keep)\n",
      "[youtube] Extracting URL: https://youtube.com/watch?v=yJzjyYL8l5Y\n",
      "[youtube] yJzjyYL8l5Y: Downloading webpage\n",
      "[youtube] yJzjyYL8l5Y: Downloading tv client config\n",
      "[youtube] yJzjyYL8l5Y: Downloading tv player API JSON\n",
      "[youtube] yJzjyYL8l5Y: Downloading ios player API JSON\n",
      "[youtube] yJzjyYL8l5Y: Downloading player 65578ad1-main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/65578ad1/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] yJzjyYL8l5Y: nsig extraction failed: Some formats may be missing\n",
      "         n = VQZhLm0rVjsGjns ; player = https://www.youtube.com/s/player/65578ad1/player_ias.vflset/en_US/base.js\n",
      "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
      "WARNING: [youtube] yJzjyYL8l5Y: nsig extraction failed: Some formats may be missing\n",
      "         n = uEGIKLKz-FVeqgj ; player = https://www.youtube.com/s/player/65578ad1/player_ias.vflset/en_US/base.js\n",
      "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
      "WARNING: [youtube] yJzjyYL8l5Y: nsig extraction failed: Some formats may be missing\n",
      "         n = LaS6NwN67Kti8rb ; player = https://www.youtube.com/s/player/65578ad1/player_ias.vflset/en_US/base.js\n",
      "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
      "WARNING: [youtube] yJzjyYL8l5Y: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] yJzjyYL8l5Y: Downloading m3u8 information\n",
      "[info] yJzjyYL8l5Y: Downloading 1 format(s): 234\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 21\n",
      "[download] Destination: /home/ramon/tts/dataset/wavs/1.mp4\n",
      "[download] 100% of    1.58MiB in 00:00:00 at 5.34MiB/s                   \n",
      "[ExtractAudio] Destination: /home/ramon/tts/dataset/wavs/1.wav\n",
      "Deleting original file /home/ramon/tts/dataset/wavs/1.mp4 (pass -k to keep)\n",
      "Detected 2 wav files. Setting MAX_WORKERS = 1\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"base\")\n",
    "with open(f\"{DATASET_DIR}/metadata.csv\", \"w\") as meta:\n",
    "    for i, yid in enumerate(YOUTUBE_IDS):\n",
    "        url = f\"https://youtube.com/watch?v={yid}\"\n",
    "        out_path = f\"{WAV_DIR}/{i}.%(ext)s\"\n",
    "\n",
    "        subprocess.run(\n",
    "            [\"yt-dlp\", \"-x\", \"--audio-format\", \"wav\", \"-o\", out_path, url], check=True\n",
    "        )\n",
    "\n",
    "        wav_path = f\"{WAV_DIR}/{i}.wav\"\n",
    "        if not os.path.exists(wav_path):\n",
    "            raise FileNotFoundError(f\"Expected wav file not found: {wav_path}\")\n",
    "\n",
    "        result = model.transcribe(wav_path)\n",
    "        meta.write(f\"wavs/{i}.wav|{result['text'].strip()}\\n\")\n",
    "\n",
    "num_utterances = len([f for f in os.listdir(WAV_DIR) if f.endswith(\".wav\")])\n",
    "MAX_WORKERS = max(1, num_utterances // 2)\n",
    "print(f\"Detected {num_utterances} wav files. Setting MAX_WORKERS = {MAX_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b926bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ramon/tts/third_party/TextyMcSpeechy/tts_dojo/DATASETS/custom_voice'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst = f\"{TEXTY_PATH}/tts_dojo/DATASETS/{VOICE_NAME}\"\n",
    "if os.path.exists(dst):\n",
    "    shutil.rmtree(dst)\n",
    "shutil.copytree(DATASET_DIR, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6957d88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ramon/tts/third_party/TextyMcSpeechy/tts_dojo/DATASETS/custom_voice\n",
      "FFMPEG OK!\n",
      "\u001b[H\u001b[2J    TextyMcSpeechy Dataset creator\n",
      "\n",
      "    This tool will perform the following operations on files in custom_voice:\n",
      "\n",
      "    1. Scan custom_voice and its subdirectories for audio files\n",
      "    2. Verify that file extensions match the contents of the files\n",
      "    3. Move audio files into folders classified by file format and sampling rate\n",
      "    4. Move any non-audio files to 'not_audio' directory\n",
      "    5. Remove empty directories\n",
      "    6. Select the highest sampling rate you have provided and resample to wav_22050 and wav_16000 (The formats piper needs)\n",
      "    7. Remove duplicate files\n",
      "    8. Check that all files referenced in metadata.csv exist.\n",
      "    9. Create dataset.conf file, which TextyMcSpeechy uses to configure your dojo.\n",
      "\n",
      "    This tool will make changes to the files in custom_voice that cannot be undone.\n",
      "    It is HIGHLY RECOMMENDED that you keep a backup of your original dataset files.\n",
      "\n",
      "    Do you wish to proceed?  (Y/N) \n",
      "    Piper uses espeak-ng to convert words in your dataset into phonemes during preprocessing.\n",
      "    Espeak-ng requires a specific identifier (eg \"en-us\") to choose which phonemes to use for the languages it supports.\n",
      "    A list of these codes is available in espeak_language_identifiers.txt.\n",
      "\n",
      "    Would you like to view the list now? [y/n]:  \n",
      "    What is the espeak-ng identifier for the language used in this dataset? (to show the list again, use \"S\"): \n",
      "          Espeak language identifier for this language is set to: en-us\n",
      "               Looked up code to build piper-compliant file name: en_US\n",
      "These values will be saved to dataset.conf in your dataset folder.\n",
      "\n",
      "    What name would you like to give this dataset?  (required): \n",
      "    Describe the contents of this dataset? (optional)\n",
      "\n",
      "     \n",
      "    TextyMcSpeechy uses pretrained piper checkpoint files as the basis for the voices it creates.\n",
      "    Which type of voice should be the default base voice for this dataset?\n",
      "\n",
      "        [M] traditionally masculine voice\n",
      "        [F] traditionally feminine voice\n",
      "         \n",
      "Classifying and moving files\n",
      "\n",
      "Processed 3 of 3 files...\n",
      "Removing empty directories\n",
      "\n",
      "\n",
      "Finding best available sampling rate\n",
      "\n",
      "Highest sampling rate: 44100\n",
      "Highest rate folders found:\n",
      "  /home/ramon/tts/third_party/TextyMcSpeechy/tts_dojo/DATASETS/custom_voice/wav_44100\n",
      "\n",
      "Ensuring wav_22050 and wav_16000 exist, please wait.\n",
      "\n",
      "\n",
      "Generating wav_16000 files from /home/ramon/tts/third_party/TextyMcSpeechy/tts_dojo/DATASETS/custom_voice/wav_44100\n",
      "\n",
      "\n",
      "Generating wav_22050 files from /home/ramon/tts/third_party/TextyMcSpeechy/tts_dojo/DATASETS/custom_voice/wav_44100\n",
      "\n",
      "\n",
      "Verifying that files in metadata.csv exist\n",
      "\n",
      "MISSING FILE: custom_voice/wav_16000/wavs/0.wav.wav\n",
      "MISSING FILE: custom_voice/wav_16000/wavs/1.wav.wav\n",
      "Checked wav_16000 for files in metadata.csv:  FAIL\n",
      "\n",
      "\n",
      "MISSING FILE: custom_voice/wav_22050/wavs/0.wav.wav\n",
      "MISSING FILE: custom_voice/wav_22050/wavs/1.wav.wav\n",
      "Checked wav_22050 for files in metadata.csv:  FAIL\n",
      "\n",
      "\n",
      "\n",
      "creating dataset.conf\n",
      "Dataset successfully created.\n"
     ]
    }
   ],
   "source": [
    "PROCEED = \"Y\"\n",
    "VIEW_LIST = \"n\"\n",
    "LANG = \"en-us\"\n",
    "DESCRIPTION = \"Custom dataset\"\n",
    "VOICE_TYPE = \"M\"\n",
    "\n",
    "os.chdir(f\"{TEXTY_PATH}/tts_dojo/DATASETS\")\n",
    "subprocess.run(\n",
    "    [\n",
    "        \"bash\",\n",
    "        \"-c\",\n",
    "        f\"./create_dataset.sh {VOICE_NAME} <<EOF\\n\"\n",
    "        f\"{PROCEED}\\n\"\n",
    "        f\"{VIEW_LIST}\\n\"\n",
    "        f\"{LANG}\\n\"\n",
    "        f\"{VOICE_NAME}\\n\"\n",
    "        f\"{DESCRIPTION}\\n\"\n",
    "        f\"{VOICE_TYPE}\\n\"\n",
    "        f\"EOF\",\n",
    "    ],\n",
    "    check=True,\n",
    ")\n",
    "os.chdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89720370",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FINAL_DIR = f\"{TEXTY_PATH}/tts_dojo/DATASETS/{VOICE_NAME}\"\n",
    "META_PATH = f\"{DATASET_FINAL_DIR}/metadata.csv\"\n",
    "\n",
    "with open(META_PATH, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# keep only filename before '|'\n",
    "# docker containers being a damn hassle. piper tts_dojo/(name)_dojo/\n",
    "lines = [f\"{line.split('/')[-1]}\" for line in lines]  # strip path\n",
    "lines = [\n",
    "    line if \"|\" in line else line.replace(\"\\n\", f\"|{TARGET_SAMPLE}\\n\")\n",
    "    for line in lines\n",
    "]\n",
    "\n",
    "# make sure each line is \"<filename>|<text>\"\n",
    "fixed = []\n",
    "for line in lines:\n",
    "    parts = line.strip().split(\"|\", 1)\n",
    "    fname = parts[0].split(\"/\")[-1]\n",
    "    text = parts[1] if len(parts) > 1 else \"\"\n",
    "    fixed.append(f\"{fname}|{text}\\n\")\n",
    "\n",
    "\n",
    "with open(META_PATH, \"w\") as f:\n",
    "    f.writelines(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb881f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********************************************************************\n",
      " A URL for M_voice, medium quality was supplied by en-us.conf \n",
      "\n",
      "\n",
      "default/M_voice/medium/epoch=4641-step=3104302.ckpt already exists.  Not downloading.\n",
      "\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      " A URL for F_voice, low quality was supplied by en-us.conf \n",
      "\n",
      "\n",
      "default/F_voice/low/epoch=2307-step=558536.ckpt already exists.  Not downloading.\n",
      "\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      " A URL for F_voice, medium quality was supplied by en-us.conf \n",
      "\n",
      "\n",
      "default/F_voice/medium/epoch=1000-step=11111111.ckpt already exists.  Not downloading.\n",
      "\n",
      "\n",
      "\n",
      "***********************************************************************\n",
      " A URL for F_voice, high quality was supplied by en-us.conf \n",
      "\n",
      "\n",
      "default/F_voice/high/epoch=2000-step=11111111.ckpt already exists.  Not downloading.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SUMMARY OF AVAILABLE PRETRAINED CHECKPOINTS - PLEASE READ\n",
      "\n",
      "    en-us.conf included pretrained checkpoints for the following combinations of voice type and quality level:\n",
      "\n",
      "            M - medium quality\n",
      "            F - low quality\n",
      "            F - medium quality\n",
      "            F - high quality\n",
      "\n",
      "    If the combination of voice type and quality level you wish to use is not listed, your options are to:\n",
      "        - train from scratch\n",
      "        - copy any properly named .ckpt file to the correct subfolder matching its voice type and quality level within PRETRAINED_CHECKPOINTS/default\n",
      "        - the .ckpt file must be named using this pattern (any name in this pattern will work, pattern is case sensitive):\n",
      "                          epoch=1000-step=11111111111.ckpt\n",
      "        - you can download a .ckpt from another language at https://huggingface.co/datasets/rhasspy/piper-checkpoints/tree/main\n",
      "        - you can also use any of the .ckpt files generated by your own training sessions (saved in your voice dojo's voice_checkpoints folder)\n",
      "\n",
      "Exiting.\n"
     ]
    }
   ],
   "source": [
    "os.chdir(f\"{TEXTY_PATH}/tts_dojo/PRETRAINED_CHECKPOINTS\")\n",
    "subprocess.run([\"./download_defaults.sh\", \"en-us\"], check=True)\n",
    "os.chdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc50442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dojo already exists.\n"
     ]
    }
   ],
   "source": [
    "os.chdir(f\"{TEXTY_PATH}/tts_dojo\")\n",
    "dojo_path = f\"{TEXTY_PATH}/tts_dojo/{VOICE_NAME}_dojo\"\n",
    "if not os.path.exists(dojo_path):\n",
    "    subprocess.run([\"./newdojo.sh\", VOICE_NAME], check=True)\n",
    "else:\n",
    "    print(\"Dojo already exists.\")\n",
    "os.chdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17319e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker rm -f textymcspeechy-piper >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c53560",
   "metadata": {},
   "source": [
    "You have to run `./scripts/training` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379ba6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"voices\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "trained_files = glob.glob(f\"{dojo_path}/tts_voices/*.onnx*\")\n",
    "for f in trained_files:\n",
    "    shutil.copy(f, f\"{BASE_DIR}/{output_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
